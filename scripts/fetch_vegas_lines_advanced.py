"""
Advanced NFL Vegas Lines Scraper
Automatically fetches real Vegas lines from multiple sources
"""

import json
import requests
from datetime import datetime, timedelta
from pathlib import Path
from bs4 import BeautifulSoup
import re
import time

OUTPUT_DIR = Path("public/data")
OUTPUT_FILE = OUTPUT_DIR / "vegas_lines.json"

TEAM_NAMES = {
    "ARI": "Cardinals", "ATL": "Falcons", "BAL": "Ravens", "BUF": "Bills",
    "CAR": "Panthers", "CHI": "Bears", "CIN": "Bengals", "CLE": "Browns",
    "DAL": "Cowboys", "DEN": "Broncos", "DET": "Lions", "GB": "Packers",
    "HOU": "Texans", "IND": "Colts", "JAX": "Jaguars", "KC": "Chiefs",
    "LAC": "Chargers", "LAR": "Rams", "LV": "Raiders", "MIA": "Dolphins",
    "MIN": "Vikings", "NE": "Patriots", "NO": "Saints", "NYG": "Giants",
    "NYJ": "Jets", "PHI": "Eagles", "PIT": "Steelers", "SF": "49ers",
    "SEA": "Seahawks", "TB": "Buccaneers", "TEN": "Titans", "WAS": "Commanders"
}

TEAM_ABBR = {v: k for k, v in TEAM_NAMES.items()}


def scrape_espn_betting_lines() -> dict:
    """Scrape actual Vegas lines from ESPN."""
    print("  Scraping ESPN for Vegas betting lines...")
    try:
        url = "https://www.espn.com/nfl/schedule"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return {}
        
        soup = BeautifulSoup(response.content, 'html.parser')
        games = {}
        
        # Look for game rows with odds information
        rows = soup.find_all('tr', class_=re.compile('Table__TR'))
        
        for row in rows[:12]:  # Get next ~12 games
            try:
                # Extract team abbreviations
                team_links = row.find_all('a', class_=re.compile('tc|Table__Team'))
                if len(team_links) >= 2:
                    # Get team names
                    away_elem = team_links[0].get_text(strip=True)
                    home_elem = team_links[1].get_text(strip=True) if len(team_links) > 1 else ""
                    
                    # Extract team abbreviation (last word usually)
                    away_abbr = away_elem.split()[-1].upper() if away_elem else ""
                    home_abbr = home_elem.split()[-1].upper() if home_elem else ""
                    
                    # Validate abbreviations
                    if len(away_abbr) <= 3 and len(home_abbr) <= 3 and away_abbr and home_abbr:
                        # Try to extract odds
                        odds_text = row.get_text()
                        
                        # Look for O/U pattern
                        ou_match = re.search(r'O/U\s*[\(]?(\d+\.?\d*)', odds_text)
                        over_under = float(ou_match.group(1)) if ou_match else None
                        
                        # Look for spread pattern
                        spread_match = re.search(r'[-+]\d+\.?\d*', odds_text)
                        spread = float(spread_match.group(0)) if spread_match else None
                        
                        game_key = f"{away_abbr}_{home_abbr}"
                        games[game_key] = {
                            "away_team": away_abbr,
                            "home_team": home_abbr,
                            "over_under": over_under,
                            "spread": spread
                        }
            except Exception as e:
                continue
        
        if games:
            print(f"    Found {len(games)} games with odds")
            return games
    except Exception as e:
        print(f"    ESPN scrape failed: {e}")
    
    return {}


def scrape_reddit_nfl_lines() -> dict:
    """Scrape Vegas lines from Reddit r/sportsbook."""
    print("  Scraping Reddit r/sportsbook for lines...")
    try:
        # Search for recent NFL odds posts
        url = "https://www.reddit.com/r/sportsbook/search.json"
        params = {
            "q": "NFL lines odds",
            "sort": "new",
            "limit": 10,
            "type": "posts"
        }
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        
        response = requests.get(url, params=params, headers=headers, timeout=10)
        if response.status_code != 200:
            return {}
        
        data = response.json()
        posts = data.get("data", {}).get("children", [])
        
        games = {}
        for post in posts[:5]:
            try:
                title = post.get("data", {}).get("title", "").upper()
                
                # Extract team abbreviations and odds from title
                team_pattern = r'\b([A-Z]{2,3})\b'
                teams = re.findall(team_pattern, title)
                
                if len(teams) >= 2:
                    away, home = teams[0], teams[1]
                    
                    # Extract over/under
                    ou_match = re.search(r'O/U[\s:]*(\d+\.?\d*)', title)
                    over_under = float(ou_match.group(1)) if ou_match else None
                    
                    game_key = f"{away}_{home}"
                    games[game_key] = {
                        "away_team": away,
                        "home_team": home,
                        "over_under": over_under,
                        "spread": None
                    }
            except:
                continue
        
        if games:
            print(f"    Found {len(games)} games on Reddit")
            return games
    except Exception as e:
        print(f"    Reddit scrape failed: {e}")
    
    return {}


def scrape_vegasinsider_style() -> dict:
    """Scrape from sports betting data aggregators."""
    print("  Checking sports betting aggregators...")
    try:
        # Try to access odds from SBR or similar
        url = "https://www.covers.com/sports/nfl/matchups"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for game containers
            games = {}
            game_elements = soup.find_all('div', class_=re.compile('matchup|game'))
            
            for elem in game_elements[:8]:
                try:
                    text = elem.get_text()
                    teams = re.findall(r'\b([A-Z]{2,3})\b', text)
                    
                    if len(teams) >= 2:
                        ou_match = re.search(r'(\d+\.?\d*)\s*O/U', text)
                        over_under = float(ou_match.group(1)) if ou_match else None
                        
                        game_key = f"{teams[0]}_{teams[1]}"
                        games[game_key] = {
                            "away_team": teams[0],
                            "home_team": teams[1],
                            "over_under": over_under,
                            "spread": None
                        }
                except:
                    pass
            
            if games:
                print(f"    Found {len(games)} games on Covers")
                return games
    except:
        pass
    
    return {}


def get_this_weekends_games() -> dict:
    """Fetch this weekend's NFL schedule automatically."""
    print("  Fetching this weekend's NFL schedule...")
    try:
        # Use ESPN schedule to identify games
        url = "https://www.espn.com/nfl/schedule"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return {}
        
        soup = BeautifulSoup(response.content, 'html.parser')
        games = {}
        
        # Find game rows
        rows = soup.find_all('tr', class_=re.compile('Table__TR'))
        
        for i, row in enumerate(rows[:16]):  # Get roughly this week's games
            try:
                # Extract teams
                team_cells = row.find_all('span', class_=re.compile('Table__Team|tc'))
                
                if len(team_cells) >= 2:
                    away_text = team_cells[0].get_text(strip=True).split()[-1].upper()
                    home_text = team_cells[1].get_text(strip=True).split()[-1].upper()
                    
                    if len(away_text) <= 3 and len(home_text) <= 3:
                        game_key = f"{away_text}_{home_text}"
                        games[game_key] = {
                            "away_team": away_text,
                            "home_team": home_text,
                            "position": i
                        }
            except:
                pass
        
        if games:
            print(f"    Identified {len(games)} games this weekend")
            return games
    except Exception as e:
        print(f"    Schedule fetch failed: {e}")
    
    return {}


def merge_games_with_odds(schedule: dict, odds: dict) -> list:
    """Merge schedule information with odds data."""
    merged = []
    
    for game_key, schedule_info in schedule.items():
        # Try to find matching odds
        odds_info = odds.get(game_key, {})
        
        merged_game = {
            "away_team": schedule_info.get("away_team"),
            "home_team": schedule_info.get("home_team"),
            "over_under": odds_info.get("over_under"),
            "spread": odds_info.get("spread"),
            "date": "2026-01-18T00:00Z"  # Approximate date
        }
        merged.append(merged_game)
    
    return merged


def get_hardcoded_games() -> list:
    """Fallback hardcoded games (actual this weekend matchups)."""
    print("  Using hardcoded games (actual Divisional Round)...")
    return [
        {
            "away_team": "HOU",
            "home_team": "NE",
            "over_under": 43.5,
            "date": "2026-01-18T15:00Z"
        },
        {
            "away_team": "BUF",
            "home_team": "DEN",
            "over_under": 47.5,
            "date": "2026-01-18T18:30Z"
        },
        {
            "away_team": "LAR",
            "home_team": "CHI",
            "over_under": 45.0,
            "date": "2026-01-19T15:00Z"
        },
        {
            "away_team": "SF",
            "home_team": "SEA",
            "over_under": 48.5,
            "date": "2026-01-19T18:30Z"
        }
    ]


def scrape_all_sources() -> list:
    """Try multiple scraping sources to get real Vegas lines."""
    print("\nðŸ“Š ATTEMPTING TO SCRAPE REAL VEGAS LINES\n")
    
    # Step 1: Get this weekend's schedule
    schedule = get_this_weekends_games()
    
    if not schedule:
        print("  Could not fetch schedule, using hardcoded games")
        return get_hardcoded_games()
    
    # Step 2: Try to get odds from multiple sources
    all_odds = {}
    
    sources = [
        scrape_espn_betting_lines,
        scrape_reddit_nfl_lines,
        scrape_vegasinsider_style,
    ]
    
    for scraper in sources:
        try:
            odds = scraper()
            all_odds.update(odds)
            time.sleep(1)  # Be respectful
        except:
            pass
    
    # Step 3: Merge schedule with odds
    if all_odds:
        merged = merge_games_with_odds(schedule, all_odds)
        if merged:
            print(f"\nâœ… Successfully scraped {len(merged)} games with real odds\n")
            return merged
    
    # Fallback
    print("\nâš ï¸  Could not get real odds, using fallback games\n")
    return get_hardcoded_games()


def parse_game_odds(game: dict) -> dict:
    """Parse game data into standard format."""
    try:
        away_abbr = game.get("away_team", "TBD").upper()
        home_abbr = game.get("home_team", "TBD").upper()
        
        game_info = {
            "id": f"{away_abbr}-{home_abbr}",
            "date": game.get("date", "2026-01-18T00:00Z"),
            "home_team": home_abbr,
            "home_team_name": TEAM_NAMES.get(home_abbr, home_abbr),
            "away_team": away_abbr,
            "away_team_name": TEAM_NAMES.get(away_abbr, away_abbr),
            "over_under": game.get("over_under"),
            "spread": game.get("spread"),
            "status": "Scheduled"
        }
        return game_info
    except Exception as e:
        print(f"    Error parsing game: {e}")
        return None


def main():
    """Main execution."""
    print("\n" + "="*70)
    print("NFL VEGAS LINES ADVANCED SCRAPER")
    print("="*70)
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Try to scrape real Vegas lines
    games = scrape_all_sources()
    
    # Parse games
    print("STEP 2: PARSING GAMES")
    print("-" * 40)
    parsed_games = []
    for game in games:
        parsed = parse_game_odds(game)
        if parsed:
            parsed_games.append(parsed)
            print(f"  {parsed['away_team_name']} @ {parsed['home_team_name']}")
            if parsed.get('over_under'):
                print(f"    Over/Under: {parsed['over_under']}")
    
    # Export to JSON
    print("\nSTEP 3: EXPORTING TO JSON")
    print("-" * 40)
    
    output_data = {
        "generatedAt": datetime.now().isoformat(),
        "gamesCount": len(parsed_games),
        "games": parsed_games
    }
    
    with open(OUTPUT_FILE, "w") as f:
        json.dump(output_data, f, indent=2)
    
    print(f"  Exported to {OUTPUT_FILE}")
    print(f"  Total games: {len(parsed_games)}")
    
    # Print summary
    print("\n" + "="*70)
    print("GAMES FOR THIS WEEKEND")
    print("="*70)
    for game in parsed_games:
        print(f"  {game['away_team_name']} @ {game['home_team_name']}")
        if game.get('over_under'):
            print(f"    Over/Under: {game['over_under']}")


if __name__ == "__main__":
    main()
